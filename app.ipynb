{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "84a3a902-75d0-4b08-87e1-6ca1362bcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.12/site-packages/gensim/models/ldamodel.py:847: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Pipeline and NLP Analysis (news_analysis_pipeline.py)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Point to db.env \n",
    "env_path = Path.cwd() / \"db.env\"    \n",
    "\n",
    "# 2) Load to override ensures fresh values)\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "\n",
    "# load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "#Load data\n",
    "df = pd.read_sql(\"SELECT * FROM car_news;\", engine)\n",
    "\n",
    "custom_stopwords = set(stopwords.words('english')) | {\n",
    "    'car','cars','vehicle','vehicles','drive','driving','review','reviews',\n",
    "    'none','one','still','even','like','well','much','better','good','also'\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", str(text))   # remove links\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)    # keep letters only\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t not in custom_stopwords and len(t) > 2]\n",
    "\n",
    "df['cleaned_content'] = df['content'].astype(str).apply(clean_text)\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def get_sentiment_label(score):\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "\n",
    "df['sentiment_score'] = df['cleaned_content'].apply(get_sentiment_score)\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "# Update sentiment into PostgreSQL\n",
    "df[['publication_date','sentiment_score', 'sentiment_label']].to_sql('temp_sentiments', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Topic Modeling\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    stop_words = set(stopwords.words('english')) | {\n",
    "    'car','cars','vehicle','vehicles','drive','driving','review','reviews',\n",
    "    'none','one','still','even','like','well','much','better','good','also'}\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "df['tokens'] = df['content'].astype(str).apply(preprocess)\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Assign dominant topic\n",
    "def get_dominant_topic(ldamodel, bow):\n",
    "    topics = ldamodel.get_document_topics(bow)\n",
    "    if topics:\n",
    "        return sorted(topics, key=lambda x: -x[1])[0][0]\n",
    "    return None\n",
    "\n",
    "df['dominant_topic'] = [get_dominant_topic(lda_model, bow) for bow in corpus]\n",
    "topic_keywords = {i: [word for word, _ in lda_model.show_topic(i)] for i in range(lda_model.num_topics)}\n",
    "df['topic_keywords'] = df['dominant_topic'].map(topic_keywords)\n",
    "\n",
    "# Save topics table\n",
    "df[['title', 'dominant_topic', 'topic_keywords', 'sentiment_label', 'sentiment_score']].to_sql(\n",
    "    'news_articles_topics', con=engine, index=False, if_exists='replace'\n",
    ")\n",
    "print(\"Successfully Uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e585e01-7594-40eb-87d0-2b41503a7ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9d9f4062-11bb-4ed4-9f4d-b0cc7935ac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Porsche buyers prefer engines. Who knew?</td>\n",
       "      <td>https://www.carmagazine.co.uk/car-news/industr...</td>\n",
       "      <td>Georg Kacher</td>\n",
       "      <td>2025-07-01</td>\n",
       "      <td>Car Magazine UK</td>\n",
       "      <td>After 30 golden years Porsche suddenly finds i...</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Renault 4 E-Tech UK pricing: strong value for ...</td>\n",
       "      <td>https://www.carmagazine.co.uk/car-news/first-o...</td>\n",
       "      <td>Phil McNamara</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>Car Magazine UK</td>\n",
       "      <td>The new is another electric car that looks to ...</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>New Yugo: scale model of next-gen low-cost sup...</td>\n",
       "      <td>https://www.carmagazine.co.uk/car-news/first-o...</td>\n",
       "      <td>Jake Groves</td>\n",
       "      <td>2025-05-06</td>\n",
       "      <td>Car Magazine UK</td>\n",
       "      <td>Yugo is making a valiant return! This is our f...</td>\n",
       "      <td>0.8907</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>BYD goes lux: the Denza Z9 GT is a 952bhp prob...</td>\n",
       "      <td>https://www.carmagazine.co.uk/car-news/first-o...</td>\n",
       "      <td>Curtis Moldrich</td>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>Car Magazine UK</td>\n",
       "      <td>► Meet Denza, the latest Chinese brand looking...</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Lewis Hamilton’s Ferrari era begins: can he wi...</td>\n",
       "      <td>https://www.carmagazine.co.uk/car-news/industr...</td>\n",
       "      <td>Damien Smith</td>\n",
       "      <td>2025-03-13</td>\n",
       "      <td>Car Magazine UK</td>\n",
       "      <td>When Ferrari’s new SF-25 Formula 1 car hit the...</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1           Porsche buyers prefer engines. Who knew?   \n",
       "1   2  Renault 4 E-Tech UK pricing: strong value for ...   \n",
       "2   3  New Yugo: scale model of next-gen low-cost sup...   \n",
       "3   4  BYD goes lux: the Denza Z9 GT is a 952bhp prob...   \n",
       "4   5  Lewis Hamilton’s Ferrari era begins: can he wi...   \n",
       "\n",
       "                                                link           author  \\\n",
       "0  https://www.carmagazine.co.uk/car-news/industr...     Georg Kacher   \n",
       "1  https://www.carmagazine.co.uk/car-news/first-o...    Phil McNamara   \n",
       "2  https://www.carmagazine.co.uk/car-news/first-o...      Jake Groves   \n",
       "3  https://www.carmagazine.co.uk/car-news/first-o...  Curtis Moldrich   \n",
       "4  https://www.carmagazine.co.uk/car-news/industr...     Damien Smith   \n",
       "\n",
       "  publication_date           source  \\\n",
       "0       2025-07-01  Car Magazine UK   \n",
       "1       2025-05-28  Car Magazine UK   \n",
       "2       2025-05-06  Car Magazine UK   \n",
       "3       2025-04-09  Car Magazine UK   \n",
       "4       2025-03-13  Car Magazine UK   \n",
       "\n",
       "                                             content  sentiment_score  \\\n",
       "0  After 30 golden years Porsche suddenly finds i...           0.9613   \n",
       "1  The new is another electric car that looks to ...           0.9992   \n",
       "2  Yugo is making a valiant return! This is our f...           0.8907   \n",
       "3  ► Meet Denza, the latest Chinese brand looking...           0.9997   \n",
       "4  When Ferrari’s new SF-25 Formula 1 car hit the...           0.9996   \n",
       "\n",
       "  sentiment_label  \n",
       "0        positive  \n",
       "1        positive  \n",
       "2        positive  \n",
       "3        positive  \n",
       "4        positive  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(\"/Users/mac/Downloads/car_news.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1002a3-ebf2-44c5-b0c1-846d5b0a97fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "76f41db6-a12f-4c0a-af63-1526d31d8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# DB connection\n",
    "\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "df = pd.read_sql(\"SELECT * FROM car_news;\", engine)\n",
    "\n",
    "# Preprocess\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    words = word_tokenize(str(text).lower())\n",
    "    return [w for w in words if w.isalpha() and w not in stop_words]\n",
    "\n",
    "# Extract n-grams (bigrams + trigrams)\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for text in df['content'].dropna():\n",
    "    tokens = tokenize(text)\n",
    "    bigrams.update(ngrams(tokens, 2))\n",
    "    trigrams.update(ngrams(tokens, 3))\n",
    "\n",
    "# Convert to DataFrame\n",
    "bigram_df = pd.DataFrame(bigrams.items(), columns=['phrase', 'count'])\n",
    "bigram_df['phrase'] = bigram_df['phrase'].apply(lambda x: ' '.join(x))\n",
    "top_bigrams = bigram_df.sort_values(by='count', ascending=False).head(50)\n",
    "# Save to DB\n",
    "top_bigrams.to_sql(\"keyword_pairs\", engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3450deb-9d46-40c3-9de0-9a7899c2363e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "54340d48-6022-4f89-b665-8caaa9813c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ner_analysis.py\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# Connect to DB\n",
    "\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "# Load article data\n",
    "df = pd.read_sql(\"SELECT * FROM car_news;\", engine)\n",
    "\n",
    "# Extract named entities from content\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['named_entities'] = df['content'].astype(str).apply(extract_entities)\n",
    "\n",
    "# Flatten all entities for frequency analysis\n",
    "all_ents = [ent for sublist in df['named_entities'] for ent in sublist]\n",
    "\n",
    "# Convert to DataFrame\n",
    "entity_df = pd.DataFrame(all_ents, columns=['entity', 'label'])\n",
    "entity_counts = entity_df.groupby(['label', 'entity']).size().reset_index(name='count')\n",
    "\n",
    "# Save top 50 entities of selected types\n",
    "filtered = entity_counts[entity_counts['label'].isin(['ORG', 'GPE', 'PRODUCT'])]\\\n",
    "           .sort_values(by='count', ascending=False).head(100)\n",
    "\n",
    "filtered.to_sql(\"named_entities_summary\", engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c636739-9140-4346-a13e-647659cb440d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "48a4c872-c4c0-4274-8439-4944788acc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# DB connection\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "\n",
    "# Load car reviews data from PostgreSQL\n",
    "df = pd.read_sql(\"SELECT * FROM car_news;\", engine)\n",
    "\n",
    "# content column entity extraction\n",
    "df['content'] = df['content'].astype(str).fillna(\"\")\n",
    "\n",
    "# Function to extract named entities from text\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text.strip(), ent.label_) for ent in doc.ents if ent.label_ in ['ORG', 'GPE', 'PRODUCT']]\n",
    "\n",
    "# Apply entity extraction\n",
    "df['named_entities'] = df['content'].apply(extract_entities)\n",
    "\n",
    "# Flatten all named entities\n",
    "all_entities = [ent for sublist in df['named_entities'] for ent in sublist]\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "entity_df = pd.DataFrame(all_entities, columns=['entity', 'label'])\n",
    "\n",
    "# Group and count occurrences\n",
    "entity_counts = entity_df.groupby(['label', 'entity']).size().reset_index(name='count')\n",
    "\n",
    "# Get top 50 entities of selected types\n",
    "top_entities = entity_counts.sort_values(by='count', ascending=False).head(100)\n",
    "\n",
    "# Save results to database\n",
    "top_entities.to_sql(\"car_news_named_entities\", engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd148147-c4ac-4cee-905a-9b561d5659bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0861651-5b02-4e24-a94b-f948b9e67cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfeb35-91cb-4f83-8701-0ef2b20eda5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b91a19b-fe83-4bfd-bade-206d464c67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Car Review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d03dfd1a-cb4e-4997-877b-9c5c6b7445c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# Load car reviews data from PostgreSQL\n",
    "df = pd.read_sql(\"SELECT * FROM car_review;\", engine)\n",
    "\n",
    "# Verdict column entity extraction\n",
    "df['verdict'] = df['verdict'].astype(str).fillna(\"\")\n",
    "\n",
    "# Function to extract named entities from text\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text.strip(), ent.label_) for ent in doc.ents if ent.label_ in ['ORG', 'GPE', 'PRODUCT']]\n",
    "\n",
    "# Apply entity extraction\n",
    "df['named_entities'] = df['verdict'].apply(extract_entities)\n",
    "\n",
    "# Flatten all named entities\n",
    "all_entities = [ent for sublist in df['named_entities'] for ent in sublist]\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "entity_df = pd.DataFrame(all_entities, columns=['entity', 'label'])\n",
    "\n",
    "# Group and count occurrences\n",
    "entity_counts = entity_df.groupby(['label', 'entity']).size().reset_index(name='count')\n",
    "\n",
    "# Get top 50 entities of selected types\n",
    "top_entities = entity_counts.sort_values(by='count', ascending=False).head(100)\n",
    "\n",
    "# Save results to database\n",
    "top_entities.to_sql(\"car_reviews_named_entities\", engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68d6c2-e60b-43e0-af8e-d1c4838f42e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "01039d21-4198-40d3-b8d0-f8f193353fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment labels added and table updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#DB connection\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# Load the table\n",
    "df = pd.read_sql(\"SELECT * FROM car_review;\", engine)\n",
    "\n",
    "# Return clean string or empty string for None/NaN\n",
    "def safe_text(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    t = safe_text(text)\n",
    "    if not t:                \n",
    "        return 0.0\n",
    "    return analyzer.polarity_scores(t)[\"compound\"]\n",
    "\n",
    "def get_sentiment_label(text, pos=0.05, neg=-0.05):\n",
    "    score = get_sentiment_score(text)\n",
    "    if score >= pos:\n",
    "        return \"positive\"\n",
    "    elif score <= neg:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply \n",
    "df[\"sentiment_score\"] = df[\"verdict\"].apply(get_sentiment_score)\n",
    "df[\"sentiment_label\"] = df[\"verdict\"].apply(get_sentiment_label)\n",
    "\n",
    "# Save updated table back (overwrite or update as needed)\n",
    "df.to_sql(\"car_review\", engine, index=False, if_exists='replace')  \n",
    "\n",
    "print(\"Sentiment labels added and table updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20311512-5302-4de4-9da5-a6483e218180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e685db3c-71fd-44bb-9046-61b55f4b752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/rn4647j13zjfx809qkvrs8dc0000gn/T/ipykernel_72085/367611976.py:13: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  trend_df = df.groupby(df['publication_date'].dt.to_period('M')).agg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to PostgreSQL\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "def get_sentiment_trend(engine):\n",
    "    df = pd.read_sql(\"SELECT publication_date, sentiment_score, sentiment_label FROM car_review\", engine)\n",
    "    df['publication_date'] = pd.to_datetime(df['publication_date'], utc=True)\n",
    "    \n",
    "    trend_df = df.groupby(df['publication_date'].dt.to_period('M')).agg(\n",
    "        avg_sentiment=('sentiment_score', 'mean'),\n",
    "        positive=('sentiment_label', lambda x: (x == 'positive').sum()),\n",
    "        negative=('sentiment_label', lambda x: (x == 'negative').sum()),\n",
    "        neutral=('sentiment_label', lambda x: (x == 'neutral').sum())\n",
    "    ).reset_index()\n",
    "\n",
    "    trend_df['publication_date'] = trend_df['publication_date'].astype(str)\n",
    "    return trend_df\n",
    "\n",
    "# Save to SQL\n",
    "trend_df = get_sentiment_trend(engine)\n",
    "trend_df.to_sql(\"sentiment_trend_monthly\", con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3b439-1070-4b0f-9585-b493422eb672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2da4c72e-80e0-4309-bd13-30aba8bcb49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/rn4647j13zjfx809qkvrs8dc0000gn/T/ipykernel_72085/3779519379.py:6: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  market_df = df.groupby(df['publication_date'].dt.to_period('M')).agg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Market trends\n",
    "def get_market_trend(engine):\n",
    "    df = pd.read_sql(\"SELECT publication_date, price, rating FROM car_review\", engine)\n",
    "    df['publication_date'] = pd.to_datetime(df['publication_date'], utc=True)\n",
    "\n",
    "    market_df = df.groupby(df['publication_date'].dt.to_period('M')).agg(\n",
    "        avg_price=('price', 'mean'),\n",
    "        avg_rating=('rating', 'mean'),\n",
    "        article_count=('price', 'count')  # Assuming every row is an article\n",
    "    ).reset_index()\n",
    "\n",
    "    market_df['publication_date'] = market_df['publication_date'].astype(str)\n",
    "\n",
    "    return market_df\n",
    "\n",
    "# Save the table\n",
    "market_df = get_market_trend(engine)\n",
    "market_df.to_sql(\"market_trend_monthly\", con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962516e3-37f6-4579-9247-e505fe19c5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2e5975e1-d67c-4595-9bff-e5b148b6ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Supervised Sentiment (TF-IDF + LR)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.000     0.000     0.000        10\n",
      "     neutral      1.000     0.977     0.989       308\n",
      "    positive      0.920     1.000     0.958       196\n",
      "\n",
      "    accuracy                          0.967       514\n",
      "   macro avg      0.640     0.659     0.649       514\n",
      "weighted avg      0.950     0.967     0.958       514\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  0   0  10]\n",
      " [  0 301   7]\n",
      " [  0   0 196]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Uses TF-IDF + Logistic Regression with class_weight='balanced' and n-grams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define features and labels\n",
    "X = df['verdict'].astype(str)\n",
    "y = df['sentiment_label']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert text into TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nSupervised Sentiment (TF-IDF + LR)\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f150178-6800-447d-adec-c116ec2f572f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8f8dd-e23c-47fa-9e96-353b61128643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2fb81-e7df-4791-a72f-d4a52d3a43d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee9237-9891-405f-ae88-adff68b99e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae98f69-3d4a-44b5-ad77-4f7722bdb39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
